---
title: "Activity 3: Bayesian Regressions"
author: "Kevin Ross (Cal Poly) and Jingchen (Monika) Hu (Vassar), eCOTS 2022"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r, warning = FALSE}

library(knitr)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)

```

```{r}

library(tidyverse)
library(bayesplot)

```

# Introduction

One commonly used class of Bayesian models in practice is Bayesian regressions. Just like classical inference with regression models, the choice of regression models depends on the nature of the outcome variable. For example,

1. Continuous outcome variable: Bayesian linear regression

2. Binary outcome variable: Bayesian logistic regression

3. Unordered categorical outcome variable: Bayesian multinomial logistic regression

4. Count outcome variable: Bayesian Poisson regression

In Bayesian inference, model parameters are considered as random and therefore one can use **prior** distributions to quantify the degree of uncertainty in these parameters. With appropriate **Markov chain Monte Carlo (MCMC)** estimation tools, one could arrive at the **posterior** distributions of these parameters, based on which one can answer relevant research questions.

In this Activity, we will introduce a sample of the Consumer Expenditure Surveys (CE) which contains 5 variables of 4 different data types. We will spend most of the energy in understanding how a Bayesian simple linear regression can be estimated using the ```brms``` package, including how to choose a prior, how to perform MCMC estimation, how to perform MCMC diagnostics, and present a selection of Bayesian inference analyses, with highlights in teaching these topics. Sample scripts for multiple linear regression, logistic regression, multinomial logistic regression, and Poisson regression are included for interested participants for further self-directed exploration.

<span style="color:red">Discussion questions are in red throughout this Activity. </span>


# The Consumer Expenditure Surveys (CE)

The CE sample comes from the 2019 Q1 CE PUMD: 5 variables, 5133 consumer units (CU).

Variable \& Description:

- **UrbanRural**: Binary; the urban / rural status of CU: 1 = Urban, 2 = Rural.

- **Income**: Continuous; the amount of CU income before taxes in past 12 months (in $USD$).

- **Race**: Categorical; the race category of the reference person: 1 = White, 2 = Black, 3 =                        Native American, 4 = Asian, 5 = Pacific Islander, 6 = Multi-race.

- **Expenditure**: Continuous; CU's total expenditures in last quarter (in $USD$).

- **KidsCount**: Count; the number of CU members under age 16. 

```{r}
CEdata <- readr::read_csv(file = "CEdata.csv")
CEdata[1:3, ]
```

# Simple linear regression

- **LogIncome**: predictor variable

- **LogExpenditure**: outcome variable

```{r}
CEdata <- CEdata %>%
  mutate(LogIncome = log(Income)) %>%
  mutate(LogExpenditure = log(Expenditure))
```

## Highlights in teaching

- This is a multi-parameter model
    - An intercept parameter $\beta_0$
    - A slope parameter $\beta_1$
    - A standard deviation parameter $\sigma$

- The Bayesian inference framework
    - Parameters are **random** variables with distributions that quantify degree of uncertainty
    - Posterior is proportional to likelihood multiplies with prior
    
- Markov chain Monte Carlo (MCMC) estimation
    - Why: The joint posterior of all parameters can be challenging to derive and MCMC estimation helps approximate the joint posterior distribution
    - How: There are many MCMC estimation software to choose from and we focus on the use of the ```brms``` package
    
- MCMC diagnostics
    - Why: We need to evaluate whether the MCMC estimation has done a reasonable job of approximating the joint posterior distribution
    - How: MCMC diagnostics tools (e.g., traceplots and autocorrelation plots) can help us check if there are convergence issues
    
- Inference
    - Credible intervals vs confidence intervals, hypothesis testing
    - Prediction
    - Model checking
    - Comparison to classical inference


## Bayesian inference with the brms package

```{r}
# make sure to install and load the library
library(brms)
```

### The model and Bayesian inference framework

\begin{eqnarray}
Y_i \mid \mu_i, \sigma &\overset{ind}{\sim}& \textrm{Normal}(\mu_i, \sigma), \\
\mu_i &=& \beta_0 + \beta_1 x_i.
\end{eqnarray}

- Model parameters: $\{\beta_0, \beta_1, \sigma\}$

- Bayesian inference: 
    - Prior for $\{\beta_0, \beta_1, \sigma\}$
    - Sampling model for $Y_1, \cdots, Y_n \mid \mu_i, \sigma$
    - Posterior for $\{\beta_0, \beta_1, \sigma\}$ using MCMC estimation

<span style="color:red">Discussion question: How many parameters and what are they in this simple linear regression model? What types of prior distributions you would like to give to them and why?</span>


### Choosing a prior

Check the default prior provided by the ```brms``` package.

```{r}
brms::get_prior(data = CEdata,
                family = gaussian,
                formula = LogExpenditure ~ 1 + LogIncome)
```

One can also use one's own priors, e.g., using the following in the ```brm()``` function, which gives a Normal(0, 10) prior for $\beta_0$, a Normal(0, 10) prior for $\beta_1$, and a Caucy(0, 1) prior for $\sigma$.

```{r, eval = FALSE}
prior = c(prior(normal(0, 10), class = Intercept),
          prior(normal(0, 10), class = b),
          prior(cauchy(0, 1), class = sigma))
```

### MCMC estimation

```{r results = 'hide'}
SLR_Bayes_fit <- brms::brm(data = CEdata,
                           family = gaussian,
                           formula = LogExpenditure ~ 1 + LogIncome,
                           iter = 5000,
                           warmup = 3000,
                           thin = 1,
                           chains = 1,
                           seed = 720)
```

**Important concepts**

- **iter**: the number of MCMC iterations
- **warmup**: the number of burn-in iterations
- **thin**: thinning rate
- **chains**: the number of MCMC chains
- **seed**: the seed

### MCMC diagnostics

**Traceplots**
```{r fig.height = 3, fig.width = 6, fig.align = "center"}
bayesplot::mcmc_trace(x = SLR_Bayes_fit,
                      pars = c("b_Intercept", "b_LogIncome", "sigma"))
```

- Look for "stickiness" which indicates convergence issues

**Autocorrelation plots (acf)**

```{r fig.height = 3, fig.width = 6, fig.align = "center"}
bayesplot::mcmc_acf(x = SLR_Bayes_fit,
                    pars = c("b_Intercept", "b_LogIncome", "sigma"))
```

- Look for slow decrease of the autocorrelation which indicates convergence issues


<span style="color:red">Discussion question: How do the traceplots and ACF plots suggest about the convergence of MCMC? What if we see a slow decrease of ACF? What remedies do we have to improve the MCMC mixing?</span>

### Posterior inference

#### A Bayesian credible interval

```{r fig.height = 3, fig.width = 6, fig.align = "center"}
post_SLR <- brms::posterior_samples(x = SLR_Bayes_fit)
bayesplot::mcmc_areas(post_SLR, pars = "b_LogIncome", prob = 0.95)
```

```{r}
quantile(post_SLR$b_LogIncome, c(0.025, 0.975))
```

A 95\% **credible interval** for the slope of LogIncome is [0.34, 0.37]. In other words, there is a 95\% posterior probability that the slope of LogIncome is between 0.34 and 0.37. 

```{r}
summary(SLR_Bayes_fit)
```

##### Classical inference with the lm package {-}

```{r}
SLR_classical_fit <- lm(formula = LogExpenditure ~ 1 + LogIncome,
                        data = CEdata)

summary(SLR_classical_fit)
```

```{r}
confint.default(SLR_classical_fit)
```

A 95\% **confidence interval** of the slope of LogIncome is [0.34, 0.37].

<span style="color:red">Discussion question: Compare the Bayesian 95\% credibale interval vs the classical 95\% confidence interval. Are they similar or are they different? What does each interval represent?</span>


#### A Bayesian hypothesis test

If one wants to evaluate whether the slope of LogIncome is at least 0.35, one can calculate the posterior probability of $Pr$(slope is at least 0.35) as follows. With such a high posterior probability, the results strongly support the claim that the slope of LogIncome is at least 0.35.

```{r}
sum(post_SLR$b_LogIncome >= 0.35)/length(post_SLR$b_LogIncome)
```

#### Bayesian prediction and posterior predictive checks


- **Prediction**: ```predict(fit)```

- **Model checking (posterior predictive check)**: ```pp_check(fit)```


### Sample script for multiple linear regression

- Own prior choice

```{r, eval = FALSE}
MLR_Bayes_fit_1 <- brm(data = CEdata,
                       family = gaussian,
                       formula = LogExpenditure ~ 1 + LogIncome + as.factor(UrbanRural),
                       prior = c(prior(normal(0, 10), class = Intercept),
                                 prior(normal(0, 10), class = b),
                                 prior(cauchy(0, 1), class = sigma)),
                       iter = 5000,
                       warmup = 3000,
                       thin = 1,
                       chains = 1,
                       seed = 129)
```

- Default prior choice

```{r, eval = FALSE}
MLR_Bayes_fit_2 <- brm(data = CEdata,
                       family = gaussian,
                       formula = LogExpenditure ~ 1 + LogIncome + as.factor(UrbanRural),
                       iter = 5000,
                       warmup = 3000,
                       thin = 1,
                       chains = 1,
                       seed = 129)
```

# Other regressions
## Logistic regression

- **LogExpenditure**: predictor variable

- **UrbanRural**: outcome variable (minus 1)

<span style="color:red">Discussion question: Are you familiar with logistic regression model? How many parameters and what are they in this Bayesian logistic regression model? </span>


### Bayesian inference with the brms package

#### Choosing a prior

Check the default prior provided by the ```brms``` package.

```{r}
brms::get_prior(data = CEdata,
                family = binomial(link = "logit"),
                formula = (UrbanRural - 1) ~ 1 + LogExpenditure)
```

One can also use one's own priors.

#### MCMC estimation

```{r results = 'hide'}
logistic_Bayes_fit <- brms::brm(data = CEdata,
                                family = binomial(link = "logit"),
                                formula = (UrbanRural - 1) ~ 1 + LogExpenditure,
                                iter = 5000,
                                warmup = 3000,
                                thin = 1,
                                chains = 1,
                                seed = 257)
```

#### MCMC diagnostics

Sample script:

```{r eval = FALSE}
bayesplot::mcmc_trace(x = logistic_Bayes_fit,
                      pars = c("b_Intercept", "b_LogExpenditure"))
bayesplot::mcmc_acf(x = logistic_Bayes_fit,
                    pars = c("b_Intercept", "b_LogExpenditure"))
```

#### Posterior inference

```{r fig.height = 3, fig.width = 6, fig.align = "center"}
post_logistic <- brms::posterior_samples(x = logistic_Bayes_fit)
bayesplot::mcmc_areas(post_logistic, pars = "b_LogExpenditure", prob = 0.95)
```


```{r}
quantile(post_logistic$b_LogExpenditure, c(0.025, 0.975))
```

A 95\% **credible interval** for the slope of LogExpenditure is [-0.45, -0.22].

```{r}
summary(logistic_Bayes_fit)
```

Additional inference:

- **Prediction**: ```predict(fit)```

- **Model checking (posterior predictive check)**: ```pp_check(fit)```

### Classical inference with the glm package

```{r}
logistic_classical_fit <- glm(formula =  (UrbanRural - 1) ~ 1 + LogExpenditure,
                              family = binomial(link = "logit"),
                              data = CEdata)

summary(logistic_classical_fit)
```

```{r}
confint.default(logistic_classical_fit)
```

A 95\% **confidence interval** of the slope of LogExpenditure is [-0.45, -0.21].


## Multinomial logistic regression

- **LogIncome**: predictor variable

- **Race**: outcome variable

<span style="color:red">Discussion question: Are you familiar with multinomial logistic regression model? How many parameters and what are they in this Bayesian multinomial logistic regression model? </span>

### Bayesian inference sample script with default priors

```{r eval = FALSE}
multi_logistic_Bayes_fit <- brms::brm(data = CEdata,
                                      family = categorical(link = "logit"),
                                      Race ~ 1 + LogIncome,
                                      iter = 5000,
                                      warmup = 3000,
                                      thin = 1,
                                      chains = 1,
                                      seed = 843)
```

### Classical inference with the nnet package

```{r eval = FALSE}
library(nnet)
multi_logistic_classical_fit <- nnet::multinom(formula =  Race ~ 1 + LogIncome,
                                               data = CEdata)
```

## Poisson regression

- **LogIncome** \& **LogExpenditure**: predictor variables

- **KidsCount**: outcome variable

<span style="color:red">Discussion question: Are you familiar with Poisson regression model? How many parameters and what are they in this Bayesian Poisson regression model? </span>

### Bayesian inference sample script with default priors

```{r eval = FALSE}
Poisson_Bayes_fit <- brms::brm(data = CEdata,
                               family = poisson(link = "log"),
                               formula = KidsCount ~ 1 + LogIncome + LogExpenditure,
                               iter = 5000,
                               warmup = 3000,
                               thin = 1,
                               chains = 1,
                               seed = 853)
```

### Classical inference with the glm package

```{r eval = FALSE}
Poisson_classical_fit <- glm(formula =  KidsCount ~ 1 + LogIncome + LogExpenditure,
                             family = poisson(link = "log"),
                             data = CEdata)
```

# Additional resources

- [Simple linear regression with JAGS](https://bayesball.github.io/BOOK/simple-linear-regression.html)

- [Multiple linear regression with JAGS](https://bayesball.github.io/BOOK/bayesian-multiple-regression-and-logistic-models.html#bayesian-multiple-linear-regression)

- [Logistic regression with JAGS](https://bayesball.github.io/BOOK/bayesian-multiple-regression-and-logistic-models.html#bayesian-logistic-regression)

- [Multiple linear regression and logistic regrssion with brms](https://bayesball.github.io/BRMS/multiple-regression-and-logistic-models.html)

